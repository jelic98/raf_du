{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DU - Domaci 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ay1ta_D9ZSWk",
        "8KBLvwJ-ZjFU",
        "Px23xt5fZc4X",
        "6CaLr6PC9QxR"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-1U6c2WQhe6"
      },
      "source": [
        "path_root = '/content/drive/My Drive/Colab Notebooks/DU/data/'\n",
        "path_train = path_root + 'Corona_NLP_train.csv'\n",
        "path_test = path_root + 'Corona_NLP_test.csv'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay1ta_D9ZSWk"
      },
      "source": [
        "### Step 1: Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1u93yoebV9B"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def load_csv(path):\n",
        "    df = pd.read_csv(path, encoding='latin')\n",
        "    df = df.drop(columns=['UserName', 'ScreenName', 'Location', 'TweetAt'])\n",
        "    df = df.rename(columns={'OriginalTweet':'x', 'Sentiment':'y'})\n",
        "\n",
        "    df['y'] = df['y'].apply(lambda x: re.sub('Extremely ', '', x))\n",
        "\n",
        "    return df\n",
        "\n",
        "train, test = load_csv(path_train), load_csv(path_test)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk6O2Yot4reb",
        "outputId": "d93c437e-f411-4252-9a89-3ce5a4e7f6b3"
      },
      "source": [
        "import nltk\n",
        "\n",
        "_ = nltk.download('stopwords')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP0aMGNLgnge",
        "outputId": "9ef98a76-af55-48bc-9fbe-ad47856f4c2f"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/65/91eab655041e9e92f948cb7302e54962035762ce7b518272ed9d6b269e93/Unidecode-1.1.2-py2.py3-none-any.whl (239kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 22.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 19.1MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92kB 11.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 11.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 11.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122kB 11.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133kB 11.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 143kB 11.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153kB 11.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163kB 11.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 174kB 11.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 184kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194kB 11.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 204kB 11.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 215kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225kB 11.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235kB 11.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 11.3MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp4Kn2W139Hl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a71270-5d39-42e2-fbbe-758adf6d11b5"
      },
      "source": [
        "%%time\n",
        "\n",
        "import html\n",
        "import unidecode\n",
        "from string import ascii_lowercase\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def clean_text(df):\n",
        "    df['x'] = [html.unescape(x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'https?://\\S+', '', x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'[^\\w\\s]|\\d+', '', x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'\\s\\s+|_|\\'', ' ', x) for x in df['x']]\n",
        "    df['x'] = [x.strip().lower() for x in df['x']]\n",
        "    df['x'] = [unidecode.unidecode(x) for x in df['x']]\n",
        "\n",
        "    for c in ascii_lowercase:\n",
        "        df['x'] = [re.sub(c+'{3,}', c+c, x) for x in df['x']]\n",
        "\n",
        "    df['x'] = [regexp_tokenize(x, '\\w+') for x in df['x']]\n",
        "    df['x'] = [' '.join(w for w in x if not w in stopwords.words('english')) for x in df['x']]\n",
        "\n",
        "clean_text(train), clean_text(test)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 24.5 ms, sys: 5.32 ms, total: 29.8 ms\n",
            "Wall time: 29.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KBLvwJ-ZjFU"
      },
      "source": [
        "### Step 2: Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "GkkIbyEwxlT2",
        "outputId": "dbd624e3-877b-48eb-d3b8-61494517f3d9"
      },
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_count = Counter(df['y'])\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.pie(y_count.values(), labels=[class_trans[x] for x in y_count.keys()], autopct='%1.1f%%')\n",
        "plt.show()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-1449a4ba8072>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_trans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautopct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%1.1f%%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-1449a4ba8072>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_trans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautopct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%1.1f%%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'class_trans' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x360 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlnV1cuBX6P0"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "for c in classes:\n",
        "    x = df[df['y'] == class_trans[c]]['x'].to_string()\n",
        "    plt.imshow(WordCloud().generate(x))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px23xt5fZc4X"
      },
      "source": [
        "### Classifier 1: Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ij0E8FsdpCZ",
        "outputId": "a2176db2-6804-4c05-93f8-1327dd55f619"
      },
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model = Pipeline([('vectorizer', TfidfVectorizer()),\n",
        "                  ('clf', LogisticRegression(max_iter=500))])\n",
        "model.fit(train['x'], train['y'])\n",
        "\n",
        "y_pred = model.predict(test['x'])\n",
        "print(classification_report(test['y'], y_pred))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 53s, sys: 5min 13s, total: 9min 6s\n",
            "Wall time: 4min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CaLr6PC9QxR"
      },
      "source": [
        "### Classifier 2: BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGQ80k7Kg1vc"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cdCLDg4hIw7"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P536vxGhKap"
      },
      "source": [
        "data_train = tokenizer.batch_encode_plus(\n",
        "    train['x'],\n",
        "    truncation=True,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    padding='max_length',\n",
        "    max_length=50,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "data_test = tokenizer.batch_encode_plus(\n",
        "    test['x'],\n",
        "    truncation=True,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    padding='max_length',\n",
        "    max_length=50,\n",
        "    return_tensors='pt'\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfz3MZk_YHwf"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "train['y_encd'] = encoder.fit_transform(train['y'])\n",
        "test['y_encd'] = encoder.fit_transform(test['y'])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbDcuV9hhL2B"
      },
      "source": [
        "from torch import tensor\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "dataset_train = TensorDataset(data_train['input_ids'],\n",
        "                              data_train['attention_mask'],\n",
        "                              torch.tensor(train['y_encd'].values))\n",
        "\n",
        "dataset_test = TensorDataset(data_test['input_ids'],\n",
        "                             data_test['attention_mask'],\n",
        "                             torch.tensor(test['y_encd'].values))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwoevX3ohPDc"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train,\n",
        "                              sampler=RandomSampler(dataset_train),\n",
        "                              batch_size=128)\n",
        "\n",
        "dataloader_test = DataLoader(dataset_test,\n",
        "                             sampler=SequentialSampler(dataset_test),\n",
        "                             batch_size=128)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ01Ujg8hNP1"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0N0at6GhQz4"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 10\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(dataloader_train)*epochs)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kCXr-CUhTku"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeuhmP3qhVwX"
      },
      "source": [
        "%%time\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    loss_total = 0\n",
        "\n",
        "    for i, batch in enumerate(dataloader_train):\n",
        "        if i % 10 == 0:\n",
        "            print(f'Batch: #{i+1}')\n",
        "\n",
        "        model.zero_grad()\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        inputs = {'input_ids': batch[0].to(device),\n",
        "                  'attention_mask': batch[1].to(device),\n",
        "                  'labels': batch[2].to(device)\n",
        "                  }\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        loss_total += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f'Epoch: #{epoch+1}')\n",
        "    print(f'Loss: {loss_total}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hWk7rDmzkEp",
        "outputId": "b5d3b435-a9ab-47f1-c714-6444209b7998"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "loss_total = 0\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for batch in dataloader_test:\n",
        "    batch = tuple(b.to(device) for b in batch)\n",
        "    inputs = {'input_ids':   batch[0],\n",
        "              'attention_mask': batch[1],\n",
        "              'labels': batch[2]\n",
        "              }\n",
        "\n",
        "    with torch.no_grad():   \n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    loss_total += outputs[0].item()\n",
        "    label_ids = inputs['labels'].cpu().numpy()\n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    y_true.append(label_ids)\n",
        "    y_pred.append(logits)\n",
        "\n",
        "y_true = np.concatenate(y_true, axis=0)\n",
        "y_pred = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "y_pred = [encoder.classes_[np.argmax(y)] for y in y_pred]\n",
        "y_true = [encoder.classes_[y] for y in y_true]\n",
        "\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.07      0.02      0.04      1633\n",
            "     Neutral       0.86      0.88      0.87      1546\n",
            "    Positive       0.04      0.11      0.06       619\n",
            "\n",
            "    accuracy                           0.39      3798\n",
            "   macro avg       0.32      0.34      0.32      3798\n",
            "weighted avg       0.39      0.39      0.38      3798\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}