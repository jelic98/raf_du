{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DU - Domaci 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ay1ta_D9ZSWk",
        "8KBLvwJ-ZjFU",
        "wJlLELvehcqK",
        "Px23xt5fZc4X",
        "LMapfZlQIFBT",
        "fVj4hs_gixAC"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt3AJfUDQTqc"
      },
      "source": [
        "function ConnectButton() {\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton, 60000);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-1U6c2WQhe6"
      },
      "source": [
        "path_root = '/content/drive/My Drive/Colab Notebooks/DU/data/'\n",
        "path_train = path_root + 'Corona_NLP_train.csv'\n",
        "path_test = path_root + 'Corona_NLP_test.csv'\n",
        "path_models = path_root + 'models/'\n",
        "path_logs = path_root + 'logs/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYxMekK3h2cz"
      },
      "source": [
        "!rm -rf '{path_models}' '{path_logs}'\n",
        "!mkdir '{path_models}' '{path_logs}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay1ta_D9ZSWk"
      },
      "source": [
        "### Step 1: Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP0aMGNLgnge"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp4Kn2W139Hl"
      },
      "source": [
        "import nltk\n",
        "import html\n",
        "import unidecode\n",
        "from string import ascii_lowercase\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(df):\n",
        "    df['x'] = [html.unescape(x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'https?://\\S+', '', x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'[^\\w\\s]|\\d+', '', x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'\\s\\s+|_|\\'', ' ', x) for x in df['x']]\n",
        "    df['x'] = [x.strip().lower() for x in df['x']]\n",
        "    df['x'] = [unidecode.unidecode(x) for x in df['x']]\n",
        "\n",
        "    for c in ascii_lowercase:\n",
        "        df['x'] = [re.sub(c+'{3,}', c+c, x) for x in df['x']]\n",
        "\n",
        "    df['x'] = [regexp_tokenize(x, '\\w+') for x in df['x']]\n",
        "    df['x'] = [' '.join(w for w in x if not w in stopwords.words('english')) for x in df['x']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1u93yoebV9B"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def load_csv(path):\n",
        "    df = pd.read_csv(path, encoding='latin')\n",
        "    df = df.drop(columns=['UserName', 'ScreenName', 'Location', 'TweetAt'])\n",
        "    df = df.rename(columns={'OriginalTweet':'x', 'Sentiment':'y'})\n",
        "\n",
        "    df['y'] = df['y'].apply(lambda x: re.sub('Extremely ', '', x))\n",
        "\n",
        "    clean_text(df)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KBLvwJ-ZjFU"
      },
      "source": [
        "### Step 2: Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkkIbyEwxlT2"
      },
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_count = Counter(df['y'])\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.pie(y_count.values(), labels=[class_trans[x] for x in y_count.keys()], autopct='%1.1f%%')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlnV1cuBX6P0"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "for c in classes:\n",
        "    x = df[df['y'] == class_trans[c]]['x'].to_string()\n",
        "    plt.imshow(WordCloud().generate(x))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJlLELvehcqK"
      },
      "source": [
        "### Step 3: Pipeline construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe9ZD_KAh31Q"
      },
      "source": [
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "from tf.data import Dataset\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import TFTrainer, TFTrainingArguments\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqVs2yE5h4hu"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNQbxB2wha9B"
      },
      "source": [
        "def train(config):\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    tokenizer = config['tokenizer'].from_pretrained(config['name'])\n",
        "\n",
        "    if config['token_add_special']:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    train = load_csv(path_train)\n",
        "\n",
        "    data_train = tokenizer.batch_encode_plus(\n",
        "        train['x'].tolist(),\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=config['token_max_length'],\n",
        "        add_special_tokens=config['token_add_special'],\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    train['y_encd'] = encoder.fit_transform(train['y'])\n",
        "\n",
        "    dataset_train = Dataset.from_tensor_slices((\n",
        "        data_train,\n",
        "        train['y_encd'].tolist()\n",
        "    ))\n",
        "\n",
        "    args = TFTrainingArguments(\n",
        "        output_dir=path_models+config['name'],\n",
        "        num_train_epochs=config['num_epochs'],\n",
        "        per_device_train_batch_size=config['batch_size'],\n",
        "        warmup_steps=config['warmup_steps'],\n",
        "        weight_decay=config['weight_decay'],\n",
        "        logging_dir=path_logs+config['name'],\n",
        "        logging_steps=config['logging_steps']\n",
        "    )\n",
        "\n",
        "    with args.strategy.scope():\n",
        "        model = tokenizer = config['model'].from_pretrained(\n",
        "            config['name'],\n",
        "            num_labels=config['num_labels'])\n",
        "\n",
        "    trainer = TFTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dataset_train\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qQ4o0PHiRr-"
      },
      "source": [
        "def test(config):\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    tokenizer = config['tokenizer'].from_pretrained(config['name'])\n",
        "\n",
        "    if config['token_add_special']:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    test = load_csv(path_test)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    test['y_encd'] = encoder.fit_transform(test['y'])\n",
        "\n",
        "    model = config['model'].from_pretrained(\n",
        "        path_models+config['name'],\n",
        "        num_labels=config['num_labels'],\n",
        "        from_tf=True)\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    for i, row in test.iterrows():\n",
        "        inputs = tokenizer(row['x'], return_tensors='pt').to(device)\n",
        "        labels = torch.tensor([row['y_encd']]).unsqueeze(0).to(device)\n",
        "\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "\n",
        "        y_true.append(labels)\n",
        "        y_pred.append(logits)\n",
        "\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    y_pred = [encoder.classes_[np.argmax(y)] for y in y_pred]\n",
        "    y_true = [encoder.classes_[y] for y in y_true]\n",
        "\n",
        "    print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px23xt5fZc4X"
      },
      "source": [
        "### Classifier 1: Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ij0E8FsdpCZ"
      },
      "source": [
        "%%time\n",
        "train = load_csv(path_train)\n",
        "model = Pipeline([('vectorizer', TfidfVectorizer()),\n",
        "                  ('clf', LogisticRegression(max_iter=500))])\n",
        "model.fit(train['x'], train['y'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyD-avF5iYRL"
      },
      "source": [
        "%%time\n",
        "test = load_csv(path_test)\n",
        "y_pred = model.predict(test['x'])\n",
        "print(classification_report(test['y'], y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMapfZlQIFBT"
      },
      "source": [
        "### Classifier 2: DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2MFmdSsIDbo"
      },
      "source": [
        "config = {\n",
        "    'name': 'distilbert-base-uncased',\n",
        "    'tokenizer': DistilBertTokenizerFast,\n",
        "    'model': TFDistilBertForSequenceClassification,\n",
        "    'batch_size': 16,\n",
        "    'num_epochs': 5,\n",
        "    'num_labels': 3,\n",
        "    'warmup_steps': 500,\n",
        "    'weight_decay': 0.01,\n",
        "    'logging_steps': 10,\n",
        "    'token_max_length': 50,\n",
        "    'token_add_special': False\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcyZS4wyi_sQ"
      },
      "source": [
        "%%time\n",
        "train(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6Xx-JXNjA3c"
      },
      "source": [
        "%%time\n",
        "test(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVj4hs_gixAC"
      },
      "source": [
        "### Classifier 3: DistilGPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZidHhXafKBb"
      },
      "source": [
        "config = {\n",
        "    'name': 'distilgpt2',\n",
        "    'tokenizer': AutoTokenizer,\n",
        "    'model': TFAutoModelForSequenceClassification,\n",
        "    'batch_size': 1,\n",
        "    'num_epochs': 5,\n",
        "    'num_labels': 3,\n",
        "    'warmup_steps': 500,\n",
        "    'weight_decay': 0.01,\n",
        "    'logging_steps': 10,\n",
        "    'token_max_length': 50,\n",
        "    'token_add_special': True\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXhws5BhgLzl"
      },
      "source": [
        "%%time\n",
        "train(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITrtlk79gN14"
      },
      "source": [
        "%%time\n",
        "test(config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}