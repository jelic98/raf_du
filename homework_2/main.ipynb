{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DU - Domaci 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ay1ta_D9ZSWk",
        "8KBLvwJ-ZjFU",
        "Px23xt5fZc4X"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt3AJfUDQTqc"
      },
      "source": [
        "function ConnectButton() {\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton, 60000);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-1U6c2WQhe6"
      },
      "source": [
        "path_root = '/content/drive/My Drive/Colab Notebooks/DU/'\n",
        "path_train = path_root + 'data/Corona_NLP_train.csv'\n",
        "path_test = path_root + 'data/Corona_NLP_test.csv'\n",
        "path_models = path_root + 'models/'\n",
        "path_logs = 'logs/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYxMekK3h2cz"
      },
      "source": [
        "!rm -rf '{path_models}' '{path_logs}'\n",
        "!mkdir '{path_models}' '{path_logs}'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee7VGNYrjzun"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay1ta_D9ZSWk"
      },
      "source": [
        "### Step 1: Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP0aMGNLgnge",
        "outputId": "c49d4edc-c2d3-4848-a82e-c10575b59d2c"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/65/91eab655041e9e92f948cb7302e54962035762ce7b518272ed9d6b269e93/Unidecode-1.1.2-py2.py3-none-any.whl (239kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 22.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 20.2MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 122kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 133kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 143kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 153kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 163kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 174kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 184kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 204kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 215kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 225kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 235kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 8.5MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp4Kn2W139Hl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2bc3bcc-f751-489d-ef23-bb00619fddd9"
      },
      "source": [
        "import nltk\n",
        "import html\n",
        "import unidecode\n",
        "from string import ascii_lowercase\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(df):\n",
        "    df['x'] = [html.unescape(x) for x in df['x_orig']]\n",
        "    df['x'] = [re.sub(r'https?://\\S+', '', x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'[^\\w\\s]|\\d+', '', x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', x) for x in df['x']]\n",
        "    df['x'] = [re.sub(r'\\s\\s+|_|\\'', ' ', x) for x in df['x']]\n",
        "    df['x'] = [x.strip().lower() for x in df['x']]\n",
        "    df['x'] = [unidecode.unidecode(x) for x in df['x']]\n",
        "\n",
        "    for c in ascii_lowercase:\n",
        "        df['x'] = [re.sub(c+'{3,}', c+c, x) for x in df['x']]\n",
        "\n",
        "    df['x'] = [regexp_tokenize(x, '\\w+') for x in df['x']]\n",
        "    df['x'] = [' '.join(w for w in x if not w in stopwords.words('english')) for x in df['x']]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1u93yoebV9B"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def load_csv(path):\n",
        "    df = pd.read_csv(path, encoding='latin')\n",
        "    df = df.drop(columns=['UserName', 'ScreenName', 'Location', 'TweetAt'])\n",
        "    df = df.rename(columns={'OriginalTweet':'x_orig', 'Sentiment':'y'})\n",
        "\n",
        "    df['y'] = df['y'].apply(lambda x: re.sub('Extremely ', '', x))\n",
        "\n",
        "    clean_text(df)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KBLvwJ-ZjFU"
      },
      "source": [
        "### Step 2: Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkkIbyEwxlT2"
      },
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_count = Counter(df['y'])\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.pie(y_count.values(), labels=[class_trans[x] for x in y_count.keys()], autopct='%1.1f%%')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlnV1cuBX6P0"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "for c in classes:\n",
        "    x = df[df['y'] == class_trans[c]]['x'].to_string()\n",
        "    plt.imshow(WordCloud().generate(x))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJlLELvehcqK"
      },
      "source": [
        "### Step 3: Pipeline construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqVs2yE5h4hu"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe9ZD_KAh31Q"
      },
      "source": [
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import TFTrainer, TFTrainingArguments\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNQbxB2wha9B"
      },
      "source": [
        "def run_train(config):\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    tokenizer = config['tokenizer'].from_pretrained(config['name'])\n",
        "\n",
        "    if config['token_add_special']:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    train = load_csv(path_train)\n",
        "\n",
        "    data_train = tokenizer.batch_encode_plus(\n",
        "        train['x'].tolist(),\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=config['token_max_length'],\n",
        "        add_special_tokens=config['token_add_special'],\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    train['y_encd'] = encoder.fit_transform(train['y'])\n",
        "\n",
        "    dataset_train = tf.data.Dataset.from_tensor_slices((\n",
        "        data_train,\n",
        "        train['y_encd'].tolist()\n",
        "    ))\n",
        "\n",
        "    args = TFTrainingArguments(\n",
        "        output_dir=path_models+config['name'],\n",
        "        num_train_epochs=config['num_epochs'],\n",
        "        per_device_train_batch_size=config['batch_size'],\n",
        "        warmup_steps=config['warmup_steps'],\n",
        "        weight_decay=config['weight_decay'],\n",
        "        logging_dir=path_logs+config['name'],\n",
        "        logging_steps=config['logging_steps']\n",
        "    )\n",
        "\n",
        "    with args.strategy.scope():\n",
        "        model = tokenizer = config['model'].from_pretrained(\n",
        "            config['name'],\n",
        "            num_labels=config['num_labels'])\n",
        "\n",
        "    trainer = TFTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dataset_train\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(path_models+config['name'])"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qQ4o0PHiRr-"
      },
      "source": [
        "def run_test(config):\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    tokenizer = config['tokenizer'].from_pretrained(config['name'])\n",
        "\n",
        "    if config['token_add_special']:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    test = load_csv(path_test)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    test['y_encd'] = encoder.fit_transform(test['y'])\n",
        "\n",
        "    if config['model_from_tf']:\n",
        "        model = config['model'].from_pretrained(\n",
        "            path_models+config['name'],\n",
        "            num_labels=config['num_labels'],\n",
        "            from_tf=True)\n",
        "    else:\n",
        "        model = config['model'].from_pretrained(\n",
        "            path_models+config['name'],\n",
        "            num_labels=config['num_labels'])\n",
        "\n",
        "    if config['model_to_cuda']:\n",
        "        device = torch.device('cuda')\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    for i, row in test.iterrows():\n",
        "        if config['tensor_type'] == 'tf':\n",
        "            inputs = tokenizer(row['x'], return_tensors='tf')\n",
        "            inputs['labels'] = tf.reshape(tf.constant(1), (-1, 1))\n",
        "            outputs = model(inputs)\n",
        "            labels = inputs['labels']\n",
        "            logits = outputs.logits\n",
        "        elif config['tensor_type'] == 'pt':\n",
        "            inputs = tokenizer(row['x'], return_tensors='pt').to(device)\n",
        "            labels = torch.tensor([row['y_encd']]).unsqueeze(0).to(device)\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            logits = outputs.logits.detach().cpu().numpy()\n",
        "\n",
        "        y_true.append(labels)\n",
        "        y_pred.append(logits)\n",
        "\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "\n",
        "    y_pred = [encoder.classes_[np.argmax(y)] for y in y_pred]\n",
        "    y_true = [encoder.classes_[y] for y in y_true]\n",
        "\n",
        "    print(classification_report(y_true, y_pred))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px23xt5fZc4X"
      },
      "source": [
        "### Classifier 1: Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ij0E8FsdpCZ",
        "outputId": "bda29302-6329-4235-8e1d-ab9d70bba610"
      },
      "source": [
        "%%time\n",
        "train = load_csv(path_train)\n",
        "model = Pipeline([('vectorizer', TfidfVectorizer()),\n",
        "                  ('clf', LogisticRegression(max_iter=500))])\n",
        "model.fit(train['x'], train['y'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 57s, sys: 30.5 s, total: 2min 27s\n",
            "Wall time: 2min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyD-avF5iYRL",
        "outputId": "ddf5cf33-ab48-4471-abe4-2afd77a1f6a3"
      },
      "source": [
        "%%time\n",
        "test = load_csv(path_test)\n",
        "y_pred = model.predict(test['x'])\n",
        "print(classification_report(test['y'], y_pred))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.79      0.76      0.78      1633\n",
            "     Neutral       0.69      0.58      0.63       619\n",
            "    Positive       0.77      0.84      0.80      1546\n",
            "\n",
            "    accuracy                           0.77      3798\n",
            "   macro avg       0.75      0.73      0.74      3798\n",
            "weighted avg       0.76      0.77      0.76      3798\n",
            "\n",
            "CPU times: user 10.2 s, sys: 1.32 s, total: 11.6 s\n",
            "Wall time: 12.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMapfZlQIFBT"
      },
      "source": [
        "### Classifier 2: DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2MFmdSsIDbo"
      },
      "source": [
        "config = {\n",
        "    'name': 'distilbert-base-uncased',\n",
        "    'tokenizer': AutoTokenizer,\n",
        "    'model': TFAutoModelForSequenceClassification,\n",
        "    'batch_size': 16,\n",
        "    'num_epochs': 5,\n",
        "    'num_labels': 3,\n",
        "    'warmup_steps': 500,\n",
        "    'weight_decay': 0.01,\n",
        "    'logging_steps': 10,\n",
        "    'token_max_length': 50,\n",
        "    'token_add_special': False,\n",
        "    'model_from_tf': False,\n",
        "    'model_to_cuda': False,\n",
        "    'tensor_type': 'tf'\n",
        "}"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Fuoq-rkA24"
      },
      "source": [
        "%tensorboard --logdir '{path_logs}'{config['name']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcyZS4wyi_sQ"
      },
      "source": [
        "%%time\n",
        "run_train(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6Xx-JXNjA3c",
        "outputId": "5a72511a-c2de-47d6-edd0-422aaf92bb5e"
      },
      "source": [
        "%%time\n",
        "run_test(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at /content/drive/My Drive/Colab Notebooks/DU/data/models/distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['dropout_259']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/My Drive/Colab Notebooks/DU/data/models/distilbert-base-uncased and are newly initialized: ['dropout_379']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVj4hs_gixAC"
      },
      "source": [
        "### Classifier 3: DistilGPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZidHhXafKBb"
      },
      "source": [
        "config = {\n",
        "    'name': 'distilgpt2',\n",
        "    'tokenizer': AutoTokenizer,\n",
        "    'model': TFAutoModelForSequenceClassification,\n",
        "    'batch_size': 1,\n",
        "    'num_epochs': 5,\n",
        "    'num_labels': 3,\n",
        "    'warmup_steps': 500,\n",
        "    'weight_decay': 0.01,\n",
        "    'logging_steps': 10,\n",
        "    'token_max_length': 50,\n",
        "    'token_add_special': True,\n",
        "    'model_from_tf': True,\n",
        "    'model_to_cuda': True,\n",
        "    'tensor_type': 'pt'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRcHscGtkBys"
      },
      "source": [
        "%tensorboard --logdir '{path_logs}'{config['name']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXhws5BhgLzl"
      },
      "source": [
        "%%time\n",
        "run_train(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITrtlk79gN14"
      },
      "source": [
        "%%time\n",
        "run_test(config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}